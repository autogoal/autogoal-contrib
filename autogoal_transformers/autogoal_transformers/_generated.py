# AUTOGENERATED ON 2024-02-22 08:44:29.966890
## DO NOT MODIFY THIS FILE MANUALLY

from numpy import inf, nan

from autogoal.grammar import (
    ContinuousValue,
    DiscreteValue,
    CategoricalValue,
    BooleanValue,
)
from autogoal_transformers._builder import (
    PretrainedWordEmbedding,
    PretrainedSequenceEmbedding,
    PretrainedTextGeneration,
    PretrainedTextClassifier,
    PretrainedTokenClassifier,
)
from autogoal.kb import *


class WORD_EMB_Bert_Base_Uncased(PretrainedWordEmbedding):
    name = "bert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Bert_Large_Uncased(PretrainedWordEmbedding):
    name = "bert-large-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Bert_Base_Cased(PretrainedWordEmbedding):
    name = "bert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Bert_Large_Cased(PretrainedWordEmbedding):
    name = "bert-large-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Bert_Base_Multilingual_Uncased(PretrainedWordEmbedding):
    name = "bert-base-multilingual-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 105879
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Bert_Base_Multilingual_Cased(PretrainedWordEmbedding):
    name = "bert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Distilbert_Base_Uncased(PretrainedWordEmbedding):
    name = "distilbert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Distilbert_Base_Cased(PretrainedWordEmbedding):
    name = "distilbert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Distilbert_Base_Multilingual_Cased(PretrainedWordEmbedding):
    name = "distilbert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Roberta_Base(PretrainedWordEmbedding):
    name = "roberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Roberta_Large(PretrainedWordEmbedding):
    name = "roberta-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Microsoft_Deberta_V3_Base(PretrainedWordEmbedding):
    name = "microsoft/deberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 128100
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Microsoft_Deberta_Base(PretrainedWordEmbedding):
    name = "microsoft/deberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta"
    architectures = None
    vocab_size = 50265
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Microsoft_Mdeberta_V3_Base(PretrainedWordEmbedding):
    name = "microsoft/mdeberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 251000
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Albert_Base_V1(PretrainedWordEmbedding):
    name = "albert-base-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Albert_Large_V1(PretrainedWordEmbedding):
    name = "albert-large-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Albert_Xlarge_V1(PretrainedWordEmbedding):
    name = "albert-xlarge-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 2048
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Albert_Xxlarge_V1(PretrainedWordEmbedding):
    name = "albert-xxlarge-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 4096
    num_attention_heads = 64

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Google_Electra_Small_Discriminator(PretrainedWordEmbedding):
    name = "google/electra-small-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 256
    num_attention_heads = 4

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Google_Electra_Base_Discriminator(PretrainedWordEmbedding):
    name = "google/electra-base-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Google_Electra_Large_Discriminator(PretrainedWordEmbedding):
    name = "google/electra-large-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Xlm_Roberta_Base(PretrainedWordEmbedding):
    name = "xlm-roberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "xlm-roberta"
    architectures = ["XLMRobertaForMaskedLM"]
    vocab_size = 250002
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class WORD_EMB_Xlm_Roberta_Large(PretrainedWordEmbedding):
    name = "xlm-roberta-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "xlm-roberta"
    architectures = ["XLMRobertaForMaskedLM"]
    vocab_size = 250002
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
    ):
        PretrainedWordEmbedding.__init__(
            self,
            batch_size=batch_size,
        )


class SEQ_EMB_Bert_Base_Uncased(PretrainedSequenceEmbedding):
    name = "bert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Bert_Large_Uncased(PretrainedSequenceEmbedding):
    name = "bert-large-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Bert_Base_Cased(PretrainedSequenceEmbedding):
    name = "bert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Bert_Large_Cased(PretrainedSequenceEmbedding):
    name = "bert-large-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Bert_Base_Multilingual_Uncased(PretrainedSequenceEmbedding):
    name = "bert-base-multilingual-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 105879
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Bert_Base_Multilingual_Cased(PretrainedSequenceEmbedding):
    name = "bert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Distilbert_Base_Uncased(PretrainedSequenceEmbedding):
    name = "distilbert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Distilbert_Base_Cased(PretrainedSequenceEmbedding):
    name = "distilbert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Distilbert_Base_Multilingual_Cased(PretrainedSequenceEmbedding):
    name = "distilbert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Roberta_Base(PretrainedSequenceEmbedding):
    name = "roberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Roberta_Large(PretrainedSequenceEmbedding):
    name = "roberta-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Microsoft_Deberta_V3_Base(PretrainedSequenceEmbedding):
    name = "microsoft/deberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 128100
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Microsoft_Deberta_Base(PretrainedSequenceEmbedding):
    name = "microsoft/deberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta"
    architectures = None
    vocab_size = 50265
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Microsoft_Mdeberta_V3_Base(PretrainedSequenceEmbedding):
    name = "microsoft/mdeberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 251000
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Albert_Base_V1(PretrainedSequenceEmbedding):
    name = "albert-base-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Albert_Large_V1(PretrainedSequenceEmbedding):
    name = "albert-large-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Albert_Xlarge_V1(PretrainedSequenceEmbedding):
    name = "albert-xlarge-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 2048
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Albert_Xxlarge_V1(PretrainedSequenceEmbedding):
    name = "albert-xxlarge-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 4096
    num_attention_heads = 64

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Google_Electra_Small_Discriminator(PretrainedSequenceEmbedding):
    name = "google/electra-small-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 256
    num_attention_heads = 4

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Google_Electra_Base_Discriminator(PretrainedSequenceEmbedding):
    name = "google/electra-base-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Google_Electra_Large_Discriminator(PretrainedSequenceEmbedding):
    name = "google/electra-large-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Xlm_Roberta_Base(PretrainedSequenceEmbedding):
    name = "xlm-roberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "xlm-roberta"
    architectures = ["XLMRobertaForMaskedLM"]
    vocab_size = 250002
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class SEQ_EMB_Xlm_Roberta_Large(PretrainedSequenceEmbedding):
    name = "xlm-roberta-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "xlm-roberta"
    architectures = ["XLMRobertaForMaskedLM"]
    vocab_size = 250002
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        pooling_strategy: CategoricalValue("mean", "max", "cls"),  # type: ignore
    ):
        PretrainedSequenceEmbedding.__init__(
            self, batch_size=batch_size, pooling_strategy=pooling_strategy
        )


class TEXT_GEN_Google_T5_T5_Small(PretrainedTextGeneration):
    name = "google-t5/t5-small"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 6
    hidden_size = 512
    num_attention_heads = 8

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Google_T5_T5_Base(PretrainedTextGeneration):
    name = "google-t5/t5-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Google_T5_T5_Large(PretrainedTextGeneration):
    name = "google-t5/t5-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Google_T5_T5_3B(PretrainedTextGeneration):
    name = "google-t5/t5-3b"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5WithLMHeadModel"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 32

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Google_T5_T5_11B(PretrainedTextGeneration):
    name = "google-t5/t5-11b"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5WithLMHeadModel"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 128

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Google_Flan_T5_Base(PretrainedTextGeneration):
    name = "google/flan-t5-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Google_Flan_T5_Large(PretrainedTextGeneration):
    name = "google/flan-t5-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Google_Flan_T5_Xxl(PretrainedTextGeneration):
    name = "google/flan-t5-xxl"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 4096
    num_attention_heads = 64

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Google_Flan_T5_Xl(PretrainedTextGeneration):
    name = "google/flan-t5-xl"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 2048
    num_attention_heads = 32

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Gpt2(PretrainedTextGeneration):
    name = "gpt2"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Gpt2_Medium(PretrainedTextGeneration):
    name = "gpt2-medium"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Gpt2_Large(PretrainedTextGeneration):
    name = "gpt2-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 36
    hidden_size = 1280
    num_attention_heads = 20

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )


class TEXT_GEN_Gpt2_Xl(PretrainedTextGeneration):
    name = "gpt2-xl"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 48
    hidden_size = 1600
    num_attention_heads = 25

    def __init__(
        self,
        batch_size: DiscreteValue(32, 1024),  # type: ignore
        max_gen_seq_length: DiscreteValue(16, 512),  # type: ignore
        temperature: ContinuousValue(0.01, 1.99),  # type: ignore
    ):
        PretrainedTextGeneration.__init__(
            self,
            batch_size=batch_size,
            max_gen_seq_length=max_gen_seq_length,
            temperature=temperature,
        )
