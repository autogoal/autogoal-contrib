# AUTOGENERATED ON 2024-07-08 15:00:36.277744
## DO NOT MODIFY THIS FILE MANUALLY

from numpy import inf, nan

from autogoal.grammar import (
    ContinuousValue,
    DiscreteValue,
    CategoricalValue,
    BooleanValue,
)
from autogoal_transformers._builder import (
    WordEmbeddingTransformer,
    TextGenerationTransformer,
    PretrainedTextClassifier,
    PretrainedTokenClassifier,
)
from autogoal.kb import *


class WORD_EMB_Bert_Base_Uncased(WordEmbeddingTransformer):
    name = "bert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Bert_Base_Cased(WordEmbeddingTransformer):
    name = "bert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Bert_Large_Uncased(WordEmbeddingTransformer):
    name = "bert-large-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Bert_Large_Cased(WordEmbeddingTransformer):
    name = "bert-large-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Bert_Base_Multilingual_Uncased(WordEmbeddingTransformer):
    name = "bert-base-multilingual-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 105879
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Bert_Base_Multilingual_Cased(WordEmbeddingTransformer):
    name = "bert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Distilbert_Base_Uncased(WordEmbeddingTransformer):
    name = "distilbert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Distilbert_Base_Cased(WordEmbeddingTransformer):
    name = "distilbert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Distilbert_Base_Multilingual_Cased(WordEmbeddingTransformer):
    name = "distilbert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Roberta_Base(WordEmbeddingTransformer):
    name = "roberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Roberta_Large(WordEmbeddingTransformer):
    name = "roberta-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Microsoft_Deberta_V3_Base(WordEmbeddingTransformer):
    name = "microsoft/deberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 128100
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Microsoft_Deberta_Base(WordEmbeddingTransformer):
    name = "microsoft/deberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta"
    architectures = None
    vocab_size = 50265
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Microsoft_Mdeberta_V3_Base(WordEmbeddingTransformer):
    name = "microsoft/mdeberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 251000
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Albert_Base_V1(WordEmbeddingTransformer):
    name = "albert-base-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Albert_Large_V1(WordEmbeddingTransformer):
    name = "albert-large-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Albert_Xlarge_V1(WordEmbeddingTransformer):
    name = "albert-xlarge-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 2048
    num_attention_heads = 16

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Albert_Xxlarge_V1(WordEmbeddingTransformer):
    name = "albert-xxlarge-v1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "albert"
    architectures = ["AlbertForMaskedLM"]
    vocab_size = 30000
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 4096
    num_attention_heads = 64

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Google_Electra_Small_Discriminator(WordEmbeddingTransformer):
    name = "google/electra-small-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 256
    num_attention_heads = 4

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Google_Electra_Base_Discriminator(WordEmbeddingTransformer):
    name = "google/electra-base-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Google_Electra_Large_Discriminator(WordEmbeddingTransformer):
    name = "google/electra-large-discriminator"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "electra"
    architectures = ["ElectraForPreTraining"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Xlm_Roberta_Base(WordEmbeddingTransformer):
    name = "xlm-roberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "xlm-roberta"
    architectures = ["XLMRobertaForMaskedLM"]
    vocab_size = 250002
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class WORD_EMB_Xlm_Roberta_Large(WordEmbeddingTransformer):
    name = "xlm-roberta-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "xlm-roberta"
    architectures = ["XLMRobertaForMaskedLM"]
    vocab_size = 250002
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        WordEmbeddingTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_T5_T5_Small(TextGenerationTransformer):
    name = "google-t5/t5-small"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 6
    hidden_size = 512
    num_attention_heads = 8

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_T5_T5_Base(TextGenerationTransformer):
    name = "google-t5/t5-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_T5_T5_Large(TextGenerationTransformer):
    name = "google-t5/t5-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_T5_T5_3B(TextGenerationTransformer):
    name = "google-t5/t5-3b"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5WithLMHeadModel"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_T5_T5_11B(TextGenerationTransformer):
    name = "google-t5/t5-11b"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5WithLMHeadModel"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 128

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_Flan_T5_Base(TextGenerationTransformer):
    name = "google/flan-t5-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_Flan_T5_Large(TextGenerationTransformer):
    name = "google/flan-t5-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_Flan_T5_Xxl(TextGenerationTransformer):
    name = "google/flan-t5-xxl"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 4096
    num_attention_heads = 64

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Google_Flan_T5_Xl(TextGenerationTransformer):
    name = "google/flan-t5-xl"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 2048
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Gpt2(TextGenerationTransformer):
    name = "gpt2"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Gpt2_Medium(TextGenerationTransformer):
    name = "gpt2-medium"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Gpt2_Large(TextGenerationTransformer):
    name = "gpt2-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 36
    hidden_size = 1280
    num_attention_heads = 20

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Gpt2_Xl(TextGenerationTransformer):
    name = "gpt2-xl"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 48
    hidden_size = 1600
    num_attention_heads = 25

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Facebook_Bart_Base(TextGenerationTransformer):
    name = "facebook/bart-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1", 2: "LABEL_2"}
    num_classes = 3
    tags = 3
    model_type = "bart"
    architectures = ["BartModel"]
    vocab_size = 50265
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Facebook_Bart_Large(TextGenerationTransformer):
    name = "facebook/bart-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1", 2: "LABEL_2"}
    num_classes = 3
    tags = 3
    model_type = "bart"
    architectures = ["BartModel"]
    vocab_size = 50265
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 12
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Microsoft_Phi_3_Mini_4K_Instruct(TextGenerationTransformer):
    name = "microsoft/Phi-3-mini-4k-instruct"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "phi3"
    architectures = ["Phi3ForCausalLM"]
    vocab_size = 32064
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 32
    hidden_size = 3072
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Microsoft_Phi_3_Medium_4K_Instruct(TextGenerationTransformer):
    name = "microsoft/Phi-3-medium-4k-instruct"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "phi3"
    architectures = ["Phi3ForCausalLM"]
    vocab_size = 32064
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 40
    hidden_size = 5120
    num_attention_heads = 40

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Microsoft_Phi_2(TextGenerationTransformer):
    name = "microsoft/phi-2"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "phi"
    architectures = ["PhiForCausalLM"]
    vocab_size = 51200
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 32
    hidden_size = 2560
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Microsoft_Phi_1_5(TextGenerationTransformer):
    name = "microsoft/phi-1_5"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "phi"
    architectures = ["PhiForCausalLM"]
    vocab_size = 51200
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 2048
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Mistralai_Mixtral_8X7B_Instruct_V01(TextGenerationTransformer):
    name = "mistralai/Mixtral-8x7B-Instruct-v0.1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "mixtral"
    architectures = ["MixtralForCausalLM"]
    vocab_size = 32000
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 32
    hidden_size = 4096
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Mistralai_Mistral_7B_V01(TextGenerationTransformer):
    name = "mistralai/Mistral-7B-v0.1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "mistral"
    architectures = ["MistralForCausalLM"]
    vocab_size = 32000
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 32
    hidden_size = 4096
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Mistralai_Mistral_7B_Instruct_V02(TextGenerationTransformer):
    name = "mistralai/Mistral-7B-Instruct-v0.2"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "mistral"
    architectures = ["MistralForCausalLM"]
    vocab_size = 32000
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 32
    hidden_size = 4096
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )


class TEXT_GEN_Mistralai_Mistral_7B_Instruct_V01(TextGenerationTransformer):
    name = "mistralai/Mistral-7B-Instruct-v0.1"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "mistral"
    architectures = ["MistralForCausalLM"]
    vocab_size = 32000
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 32
    hidden_size = 4096
    num_attention_heads = 32

    def __init__(
        self,
    ):
        TextGenerationTransformer.__init__(
            self,
        )
