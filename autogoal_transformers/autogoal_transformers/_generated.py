# AUTOGENERATED ON 2024-02-19 14:47:00.089777
## DO NOT MODIFY THIS FILE MANUALLY

from numpy import inf, nan

from autogoal.grammar import (
    ContinuousValue,
    DiscreteValue,
    CategoricalValue,
    BooleanValue,
)
from autogoal_transformers._builder import (
    PretrainedWordEmbedding,
    PretrainedSequenceEmbedding,
    PretrainedTextGeneration,
    PretrainedTextClassifier,
    PretrainedTokenClassifier,
)
from autogoal.kb import *


class WORD_EMB_BertBaseUncased(PretrainedWordEmbedding):
    name = "bert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_BertLargeUncased(PretrainedWordEmbedding):
    name = "bert-large-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_BertBaseCased(PretrainedWordEmbedding):
    name = "bert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_BertLargeCased(PretrainedWordEmbedding):
    name = "bert-large-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_BertBaseMultilingualUncased(PretrainedWordEmbedding):
    name = "bert-base-multilingual-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 105879
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_BertBaseMultilingualCased(PretrainedWordEmbedding):
    name = "bert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_DistilbertBaseUncased(PretrainedWordEmbedding):
    name = "distilbert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_DistilbertBaseCased(PretrainedWordEmbedding):
    name = "distilbert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_DistilbertBaseMultilingualCased(PretrainedWordEmbedding):
    name = "distilbert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_RobertaBase(PretrainedWordEmbedding):
    name = "roberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_RobertaLarge(PretrainedWordEmbedding):
    name = "roberta-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_Microsoft_DebertaV3Base(PretrainedWordEmbedding):
    name = "microsoft/deberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 128100
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_Microsoft_DebertaBase(PretrainedWordEmbedding):
    name = "microsoft/deberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta"
    architectures = None
    vocab_size = 50265
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class WORD_EMB_Microsoft_MdebertaV3Base(PretrainedWordEmbedding):
    name = "microsoft/mdeberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 251000
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedWordEmbedding.__init__(self, batch_size)


class SEQ_EMB_BertBaseUncased(PretrainedSequenceEmbedding):
    name = "bert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_BertLargeUncased(PretrainedSequenceEmbedding):
    name = "bert-large-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_BertBaseCased(PretrainedSequenceEmbedding):
    name = "bert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_BertLargeCased(PretrainedSequenceEmbedding):
    name = "bert-large-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_BertBaseMultilingualUncased(PretrainedSequenceEmbedding):
    name = "bert-base-multilingual-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 105879
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_BertBaseMultilingualCased(PretrainedSequenceEmbedding):
    name = "bert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "bert"
    architectures = ["BertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = 2
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_DistilbertBaseUncased(PretrainedSequenceEmbedding):
    name = "distilbert-base-uncased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 30522
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_DistilbertBaseCased(PretrainedSequenceEmbedding):
    name = "distilbert-base-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 28996
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_DistilbertBaseMultilingualCased(PretrainedSequenceEmbedding):
    name = "distilbert-base-multilingual-cased"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "distilbert"
    architectures = ["DistilBertForMaskedLM"]
    vocab_size = 119547
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 6
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_RobertaBase(PretrainedSequenceEmbedding):
    name = "roberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_RobertaLarge(PretrainedSequenceEmbedding):
    name = "roberta-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "roberta"
    architectures = ["RobertaForMaskedLM"]
    vocab_size = 50265
    type_vocab_size = 1
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_Microsoft_DebertaV3Base(PretrainedSequenceEmbedding):
    name = "microsoft/deberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 128100
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_Microsoft_DebertaBase(PretrainedSequenceEmbedding):
    name = "microsoft/deberta-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta"
    architectures = None
    vocab_size = 50265
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class SEQ_EMB_Microsoft_MdebertaV3Base(PretrainedSequenceEmbedding):
    name = "microsoft/mdeberta-v3-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "deberta-v2"
    architectures = None
    vocab_size = 251000
    type_vocab_size = 0
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedSequenceEmbedding.__init__(self, batch_size)


class TEXT_GEN_T5Small(PretrainedTextGeneration):
    name = "t5-small"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 6
    hidden_size = 512
    num_attention_heads = 8

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)


class TEXT_GEN_T5Base(PretrainedTextGeneration):
    name = "t5-base"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)


class TEXT_GEN_T5Large(PretrainedTextGeneration):
    name = "t5-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5ForConditionalGeneration"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)


class TEXT_GEN_T5B(PretrainedTextGeneration):
    name = "t5-3b"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5WithLMHeadModel"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 32

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)


class TEXT_GEN_T5B(PretrainedTextGeneration):
    name = "t5-11b"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "t5"
    architectures = ["T5WithLMHeadModel"]
    vocab_size = 32128
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = True
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 128

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)


class TEXT_GEN_Gpt2(PretrainedTextGeneration):
    name = "gpt2"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 12
    hidden_size = 768
    num_attention_heads = 12

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)


class TEXT_GEN_Gpt2Medium(PretrainedTextGeneration):
    name = "gpt2-medium"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 24
    hidden_size = 1024
    num_attention_heads = 16

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)


class TEXT_GEN_Gpt2Large(PretrainedTextGeneration):
    name = "gpt2-large"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 36
    hidden_size = 1280
    num_attention_heads = 20

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)


class TEXT_GEN_Gpt2Xl(PretrainedTextGeneration):
    name = "gpt2-xl"
    id2label = {0: "LABEL_0", 1: "LABEL_1"}
    num_classes = 2
    tags = 2
    model_type = "gpt2"
    architectures = ["GPT2LMHeadModel"]
    vocab_size = 50257
    type_vocab_size = None
    is_decoder = False
    is_encoder_decoder = False
    num_layers = 48
    hidden_size = 1600
    num_attention_heads = 25

    def __init__(self, batch_size: DiscreteValue(32, 256)): # type: ignore
        PretrainedTextGeneration.__init__(self, batch_size)
